# Ollama Configuration
# For local server (default)
OLLAMA_BASE_URL=http://localhost:11434

# For external GPU server
# OLLAMA_BASE_URL=http://your-gpu-server-ip:8001

# Model to use
OLLAMA_MODEL=llama3.2
