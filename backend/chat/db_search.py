"""
Scalable database search system for legal documents
Handles 200K+ documents efficiently
"""
from django.db.models import Q
from .models import LawDocument
from .ollama_client import OllamaClient
import re
import os
import sys
from pathlib import Path

# Load environment variables
sys.path.append(str(Path(__file__).parent.parent))
try:
    from load_env import load_env
    load_env()
except ImportError:
    pass

class LegalDBSearch:
    def __init__(self):
        # Initialize with environment-based configuration
        base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        model = os.getenv('OLLAMA_MODEL', 'llama3.2')
        self.ollama_client = OllamaClient(base_url=base_url, model=model)
        
        # Print connection info for debugging
        print(f"ğŸ”— Ollama ì—°ê²°: {base_url} (ëª¨ë¸: {model})")
    
    def search_legal_documents(self, query, limit=10):
        """Enhanced search with better ranking"""
        
        keywords = self._extract_keywords(query)
        
        # Weighted search - title matches score higher
        results = []
        
        # 1. Exact title matches (highest priority)
        for keyword in keywords:
            title_matches = LawDocument.objects.filter(
                title__icontains=keyword
            ).distinct()[:5]
            results.extend(title_matches)
        
        # 2. Content matches
        content_query = Q()
        for keyword in keywords:
            content_query |= Q(content__icontains=keyword)
        
        content_matches = LawDocument.objects.filter(content_query).distinct()[:10]
        results.extend(content_matches)
        
        # 3. Court/case number matches
        meta_query = Q()
        for keyword in keywords:
            meta_query |= (
                Q(court_name__icontains=keyword) |
                Q(case_number__icontains=keyword)
            )
        
        meta_matches = LawDocument.objects.filter(meta_query).distinct()[:5]
        results.extend(meta_matches)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_results = []
        for doc in results:
            if doc.document_id not in seen:  # Use document_id instead of id
                seen.add(doc.document_id)
                unique_results.append(doc)
        
        return unique_results[:limit]
    
    def generate_answer_with_context(self, question, search_results, recent_chats):
        """Enhanced answer generation with chat context"""
        
        if not search_results:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì‹œê±°ë‚˜ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
        
        # Prepare chat context
        context_history = ""
        if recent_chats:
            context_history = "\nã€ì´ì „ ëŒ€í™” ë‚´ìš©ã€‘\n"
            for chat in reversed(recent_chats):  # Oldest first
                context_history += f"Q: {chat.question}\nA: {chat.answer[:200]}...\n\n"
        
        # Prepare detailed legal context
        context_parts = []
        for i, doc in enumerate(search_results[:5], 1):
            context_parts.append(
                f"ã€íŒë¡€ {i}ã€‘\n"
                f"ì œëª©: {doc.title}\n"
                f"ë²•ì›: {doc.court_name}\n"
                f"ì‚¬ê±´ë²ˆí˜¸: {doc.case_number}\n"
                f"ì„ ê³ ì¼: {doc.enforcement_date}\n"
                f"ë‚´ìš©: {doc.content[:300]}...\n"
            )
        
        legal_context = "\n".join(context_parts)
        
        # Enhanced prompt with context awareness
        prompt = f"""ë‹¹ì‹ ì€ ë¯¼ì‚¬ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

{context_history}

ã€ê²€ìƒ‰ëœ ê´€ë ¨ íŒë¡€ã€‘
{legal_context}

ã€í˜„ì¬ ì§ˆë¬¸ã€‘
{question}

ã€ë‹µë³€ ì‘ì„± ì§€ì¹¨ã€‘
1. ì´ì „ ëŒ€í™”ì™€ì˜ ì—°ê´€ì„±ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€
2. ê´€ë ¨ íŒë¡€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ì—¬ ì„¤ëª…
3. ë²•ì  ìŸì ê³¼ íŒë‹¨ ê¸°ì¤€ ëª…ì‹œ
4. ì‹¤ë¬´ì  ì‹œì‚¬ì  ì œì‹œ
5. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
6. ë§¥ë½ìƒ "ê·¸ëŸ¼", "ì´ ê²½ìš°" ë“±ì˜ í‘œí˜„ì´ ìˆìœ¼ë©´ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€

ë‹µë³€:"""
        
        try:
            response = self.ollama_client.generate_response(prompt)
            
            # Add source information
            source_info = f"\n\nã€ì°¸ê³  íŒë¡€ã€‘\n"
            for i, doc in enumerate(search_results[:3], 1):
                source_info += f"{i}. {doc.title} ({doc.court_name}, {doc.enforcement_date})\n"
            
            return response + source_info
            
        except Exception as e:
            print(f"Ollama error: {e}")
            # Enhanced fallback with context
            return self._create_enhanced_summary_with_context(question, search_results, recent_chats)
    
    def _create_enhanced_summary_with_context(self, question, results, recent_chats):
        """Create enhanced summary with context awareness"""
        if not results:
            return "ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        summary = f"ã€{question}ã€‘ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼\n\n"
        
        # Add context reference if available
        if recent_chats and any(word in question for word in ['ê·¸ëŸ¼', 'ê·¸ëŸ¬ë©´', 'ì´ ê²½ìš°', 'ê·¸ê²ƒ', 'ì´ê²ƒ']):
            summary += f"â€» ì´ì „ ëŒ€í™”ì™€ ì—°ê´€ëœ ì§ˆë¬¸ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n\n"
        
        summary += f"ì´ {len(results)}ê±´ì˜ ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n\n"
        
        # Group by court for better organization
        court_groups = {}
        for doc in results[:10]:
            court = doc.court_name or "ê¸°íƒ€"
            if court not in court_groups:
                court_groups[court] = []
            court_groups[court].append(doc)
        
        for court, docs in court_groups.items():
            summary += f"â–¶ {court}\n"
            for doc in docs[:3]:  # Limit per court
                summary += f"  â€¢ {doc.title}\n"
                summary += f"    ì‚¬ê±´ë²ˆí˜¸: {doc.case_number} | ì„ ê³ ì¼: {doc.enforcement_date}\n"
            if len(docs) > 3:
                summary += f"    ... ì™¸ {len(docs) - 3}ê±´\n"
            summary += "\n"
        
        summary += "ë” êµ¬ì²´ì ì¸ ë¶„ì„ì„ ì›í•˜ì‹œë©´ íŠ¹ì • íŒë¡€ë‚˜ ë²•ì  ìŸì ì„ ëª…ì‹œí•´ ì£¼ì„¸ìš”."
        
        return summary
        """Enhanced answer generation with better prompts"""
        
        if not search_results:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì‹œê±°ë‚˜ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
        
        # Prepare detailed context
        context_parts = []
        for i, doc in enumerate(search_results[:5], 1):
            context_parts.append(
                f"ã€íŒë¡€ {i}ã€‘\n"
                f"ì œëª©: {doc.title}\n"
                f"ë²•ì›: {doc.court_name}\n"
                f"ì‚¬ê±´ë²ˆí˜¸: {doc.case_number}\n"
                f"ì„ ê³ ì¼: {doc.enforcement_date}\n"
                f"ë‚´ìš©: {doc.content[:300]}...\n"
            )
        
        context = "\n".join(context_parts)
        
        # Enhanced prompt for better legal analysis
        prompt = f"""ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ íŒë¡€ë“¤ì„ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ã€ê²€ìƒ‰ëœ ê´€ë ¨ íŒë¡€ã€‘
{context}

ã€ì§ˆë¬¸ã€‘
{question}

ã€ë‹µë³€ ì‘ì„± ì§€ì¹¨ã€‘
1. ê´€ë ¨ íŒë¡€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ì—¬ ì„¤ëª…
2. ë²•ì  ìŸì ê³¼ íŒë‹¨ ê¸°ì¤€ ëª…ì‹œ
3. ì‹¤ë¬´ì  ì‹œì‚¬ì  ì œì‹œ
4. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±

ë‹µë³€:"""
        
        try:
            response = self.ollama_client.generate_response(prompt)
            
            # Add source information
            source_info = f"\n\nã€ì°¸ê³  íŒë¡€ã€‘\n"
            for i, doc in enumerate(search_results[:3], 1):
                source_info += f"{i}. {doc.title} ({doc.court_name}, {doc.enforcement_date})\n"
            
            return response + source_info
            
        except Exception as e:
            print(f"Ollama error: {e}")
            # Enhanced fallback
            return self._create_enhanced_summary(question, search_results)
    
    def _extract_keywords(self, query):
        """Enhanced keyword extraction for legal queries"""
        
        # Legal-specific stop words
        stop_words = {
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ',
            'ìˆëŠ”', 'ì—†ëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ê´€ë ¨', 'ëŒ€í•œ', 'ì—ì„œ', 'ì—ê²Œ', 'ë¶€í„°',
            'ê¹Œì§€', 'ê°™ì€', 'ë‹¤ë¥¸', 'ì–´ë–¤', 'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ì™œ', 'ì–´ë–»ê²Œ'
        }
        
        # Important legal terms (boost these)
        legal_terms = {
            'ë¯¼ì‚¬', 'íŒê²°', 'ë²•ì›', 'ì†í•´ë°°ìƒ', 'ê³„ì•½', 'ë¶ˆë²•í–‰ìœ„', 'ì†Œìœ ê¶Œ', 'ì±„ê¶Œ', 'ì±„ë¬´',
            'ì†Œì†¡', 'íŒê²°', 'ì„ ê³ ', 'ì‚¬ê±´', 'ì¬íŒ', 'ë²•ë¥ ', 'ë¯¼ë²•', 'ìƒë²•', 'ë¶€ë™ì‚°',
            'êµí†µì‚¬ê³ ', 'ì˜ë£Œì‚¬ê³ ', 'ì„ê¸ˆ', 'í•´ê³ ', 'ê³„ì•½í•´ì§€', 'ìœ„ì•½ê¸ˆ', 'ì†í•´'
        }
        
        # Extract words
        words = re.findall(r'\b\w+\b', query)
        
        # Prioritize legal terms, then filter stop words
        keywords = []
        
        # First add legal terms found in query
        for word in words:
            if word in legal_terms:
                keywords.append(word)
        
        # Then add other meaningful words
        for word in words:
            if (len(word) > 1 and 
                word not in stop_words and 
                word not in keywords):
                keywords.append(word)
        
        return keywords[:7]  # Increased limit for better coverage
    
    def _create_enhanced_summary(self, question, results):
        """Create enhanced summary without LLM"""
        if not results:
            return "ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        summary = f"ã€{question}ã€‘ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼\n\n"
        summary += f"ì´ {len(results)}ê±´ì˜ ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n\n"
        
        # Group by court for better organization
        court_groups = {}
        for doc in results[:10]:
            court = doc.court_name or "ê¸°íƒ€"
            if court not in court_groups:
                court_groups[court] = []
            court_groups[court].append(doc)
        
        for court, docs in court_groups.items():
            summary += f"â–¶ {court}\n"
            for doc in docs[:3]:  # Limit per court
                summary += f"  â€¢ {doc.title}\n"
                summary += f"    ì‚¬ê±´ë²ˆí˜¸: {doc.case_number} | ì„ ê³ ì¼: {doc.enforcement_date}\n"
            if len(docs) > 3:
                summary += f"    ... ì™¸ {len(docs) - 3}ê±´\n"
            summary += "\n"
        
        summary += "ë” êµ¬ì²´ì ì¸ ë¶„ì„ì„ ì›í•˜ì‹œë©´ íŠ¹ì • íŒë¡€ë‚˜ ë²•ì  ìŸì ì„ ëª…ì‹œí•´ ì£¼ì„¸ìš”."
        
        return summary

# Global instance
legal_search = LegalDBSearch()
