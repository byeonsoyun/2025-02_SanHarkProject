# scrape_law.pyì—ì„œ ì‹¤í–‰ ì•ˆ ë˜ê¸¸ëž˜ ë§Œë“  ìƒˆ íŒŒì¼ì¸ë° ì´ê²ƒë„ ì•ˆ ë¨(ì‚¬ë²•ì •ë³´ê³µê°œí¬í„¸ ë¡œê·¸ì¸ ì´ìŠˆ)
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import os, sys, django

# Django í™˜ê²½ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'backend.settings')
django.setup()

from data.cases.models import LawArticle

url = "https://portal.scourt.go.kr/pgp/main.on?w2xPath=PGP1021M04&jisCntntsSrno=3335773&srchwd=*&rnum=1&c=900&pgDvs=1"

options = Options()
options.add_argument("--headless")  # ì°½ ì•ˆ ë„ìš°ê³  ì‹¤í–‰
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

driver = webdriver.Chrome(options=options)
driver.get(url)

# íŽ˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
time.sleep(5)

html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')
driver.quit()

articles = soup.find_all("div", class_="lawArticle")

print(f"ì´ {len(articles)}ê°œ ì¡°ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

for art in articles:
    num_tag = art.find(["strong", "b", "h4"])
    num = num_tag.get_text(strip=True) if num_tag else ""
    title = ""
    if "(" in num:
        title = num.split("(")[1].split(")")[0]
        num = num.split("(")[0]

    content_parts = [p.get_text(" ", strip=True) for p in art.find_all(["p", "li"]) if len(p.get_text(strip=True)) > 3]
    content = "\n".join(content_parts)

    if content:
        LawArticle.objects.create(
            law_name="ì§€ë°©ì„¸ê¸°ë³¸ë²•",
            article_num=num,
            article_title=title,
            article_content=content
        )
        print(f"âœ… {num} {title} ì €ìž¥ ì™„ë£Œ")

print("ðŸŽ‰ ëª¨ë“  ì¡°ë¬¸ ì €ìž¥ ì™„ë£Œ!")
